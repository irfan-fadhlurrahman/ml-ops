{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be3e097-69cb-4d3c-abcb-9e7dd9f10b58",
   "metadata": {},
   "source": [
    "## Batch Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb8afc-e415-4f27-bad9-9333d1654650",
   "metadata": {},
   "source": [
    "https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/04-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "633b1645-eee1-4b0c-bafa-44fb65550deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of training data: 5.700\n",
      "RMSE of validation data: 7.759\n",
      "pandas: 1.4.2\n",
      "numpy: 1.22.3\n",
      "sklearn: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import inflection\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "SEED = 0\n",
    "def major_library_version():\n",
    "    import pandas, numpy, sklearn\n",
    "    print(f\"pandas: {pandas.__version__}\")\n",
    "    print(f\"numpy: {numpy.__version__}\")\n",
    "    print(f\"sklearn: {sklearn.__version__}\")\n",
    "    \n",
    "def load_dataset(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    # duration\n",
    "    df['duration'] = df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "    \n",
    "    # pickup and dropoff location\n",
    "    categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    df.loc[:, \"PU_DO\"] = df[\"PULocationID\"] + \"_\" + df[\"DOLocationID\"]\n",
    "    \n",
    "    # every filter must be at the end of the function\n",
    "    return df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "     \n",
    "\n",
    "\n",
    "def train_the_model(train, features=None, target='duration'):\n",
    "    if features == None:\n",
    "        features = [\"PU_DO\", \"trip_distance\"]\n",
    "        \n",
    "    # preprocess the one-hot encode outside pipeline in order\n",
    "    # to take advantage of dictionary input later on web service\n",
    "    dv = DictVectorizer()\n",
    "    \n",
    "    # parse to dictionaries for using DictVectorizer\n",
    "    train_dicts = train[features].to_dict(orient=\"records\")\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    rmse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    \n",
    "    return dv, model, rmse\n",
    "\n",
    "def run_the_model(val, dv, model, features=None, target='duration'):\n",
    "    if features == None:\n",
    "        features = [\"PU_DO\", \"trip_distance\"]\n",
    "        \n",
    "    val_dicts = val[features].to_dict(orient=\"records\")\n",
    "    X_val = dv.transform(val_dicts)\n",
    "    y_val = val[target]\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    \n",
    "    return y_pred, rmse\n",
    "\n",
    "def save_the_model(model, path=\"./models/linear_regression.bin\"):\n",
    "    with open(path, 'wb') as file_out:\n",
    "         joblib.dump(model, file_out)\n",
    "\n",
    "def load_the_model(path=\"./models/linear_regression.bin\"):\n",
    "    with open(path, 'rb') as file_in:\n",
    "        return joblib.load(file_in)\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    * Use data from Jan 2021 for training then use Feb 2021 data for validation.\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    train = load_dataset(\"../dataset/green_tripdata_2021-01.parquet\")\n",
    "    train = prepare_features(train)\n",
    "    val = load_dataset(\"../dataset/green_tripdata_2021-02.parquet\")\n",
    "    val = prepare_features(val)\n",
    "    \n",
    "    # train the model\n",
    "    dv, model, rmse = train_the_model(train, features=None, target='duration')\n",
    "    print(f\"RMSE of training data: {rmse:.3f}\")\n",
    "    \n",
    "    # prepare the path\n",
    "    from datetime import datetime\n",
    "    saved_time = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    dv_path = f\"./models/{saved_time}_preprocessor.b\"\n",
    "    model_path = f\"./models/{saved_time}_lin_reg.bin\"\n",
    "    \n",
    "    # save the pipeline\n",
    "    save_the_model(dv, path=dv_path)\n",
    "    save_the_model(model, path=model_path)\n",
    "    \n",
    "    # load and test the model on validation dataset\n",
    "    dv = load_the_model(path=dv_path)\n",
    "    model = load_the_model(path=model_path)\n",
    "    y_pred, rmse = run_the_model(val, dv, model, features=None, target='duration')\n",
    "    print(f\"RMSE of validation data: {rmse:.3f}\")\n",
    "    \n",
    "    # library version\n",
    "    major_library_version()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
